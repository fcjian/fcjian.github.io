<!DOCTYPE HTML>
<html lang="en"><head>
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6J8CBRENW5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6J8CBRENW5');
</script>
  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chengjian Feng</title>
  
  <meta name="author" content="Chengjian Feng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!--link rel="icon" type="image/png" href="images/seal_icon.png"-->

</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chengjian Feng</name>
              </p>
              <p style="text-align:center">
                I am currently a researcher at Meituan Inc. 
                My primary research interests encompass a broad range of topics, including large multimodal models, diffusion models, autonomous driving, embodied AI, object detection, domain adaptation, and more. 
                Specifically, I am particularly interested in the application of <em>large multimodal models</em> and <em>diffusion models</em> to improve our daily lives.
              </p>

              <p style="text-align:center">
                <a href="https://fcjian.github.io/">Home</a> &nbsp/&nbsp
                <a href="mailto:fcjian@outlook.com">Email</a> &nbsp/&nbsp
                <!--<a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp/&nbsp-->
                <a href="https://github.com/fcjian/">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=jc3YlxEAAAAJ">Google Scholar</a>
              </p>
            </td>
            <td style="padding:8%;width:40%;max-width:40%">
              <img style="width:80%;max-width:100%" alt="profile photo" src="resources/pp.jpeg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Papers</heading>
              <!-- <p>
                
              </p> -->
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="resources/tood.png" alt="clean-usnob" width="180">
            </td>
            <td width="75%" valign="middle">
              <a href="tood/index.html">
                <papertitle>TOOD: Task-aligned One-stage Object Detection</papertitle>
              </a>
              <br>
              <strong>Chenjian Feng</strong>, <a href="https://y-zhong.info/">Yujie Zhong</a>, <a>Yu Gao</a>, <a href="https://scholar.google.com/citations?user=0isiW-4AAAAJ&hl=zh-CN&oi=ao">Matthew R. Scott</a>, <a href="http://whuang.org/">Weilin Huang</a>
              <br>
              <em>ICCV</em>, 2021
              <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="tood/index.html">project page</a> /
              <a href="https://arxiv.org/abs/2108.07755">arXiv</a>
              <p></p>
              <p>We propose a Task-aligned One-stage Object Detection (TOOD) that explicitly aligns the classification and localization tasks in a learning-based manner.
                It is widely applied by YOLO series, such as <a href="https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/configs/ppyoloe">PP-YOLOE</a>, <a href="https://github.com/meituan/YOLOv6">YOLOv6</a>, <a href="https://docs.ultralytics.com/zh">YOLOv8</a>, <a href="https://github.com/THU-MIG/yolov10">YOLOv10</a>, <a href="https://github.com/AILab-CVC/YOLO-World">YOLO-World</a>.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="resources/loce.png" alt="clean-usnob" width="180">
            </td>
            <td width="75%" valign="middle">
              <a href="loce/index.html">
                <papertitle>Exploring Classification Equilibrium in Long-Tailed Object Detection</papertitle>
              </a>
              <br>
              <strong>Chenjian Feng</strong>, <a href="https://y-zhong.info/">Yujie Zhong</a>, <a href="http://whuang.org/">Weilin Huang</a>
              <br>
              <em>ICCV</em>, 2021
              <br>
              <a href="loce/index.html">project page</a> /
              <a href="https://arxiv.org/abs/2108.07507">arXiv</a>
              <p></p>
              <p>We balance the classification of the long-tailed detector via an Equilibrium Loss (EBL) and a Memory-augmented Feature Sampling (MFS) method.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="resources/promptdet.png" alt="clean-usnob" width="180">
            </td>
            <td width="75%" valign="middle">
              <a href="promptdet/index.html">
                <papertitle>PromptDet: Towards Open-vocabulary Detection using Uncurated Images</papertitle>
              </a>
              <br>
              <strong>Chenjian Feng</strong>, <a href="https://y-zhong.info/">Yujie Zhong</a>, <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl=zh-CN&oi=ao">
							Zequn Jie</a>, <a href="https://scholar.google.com/citations?user=jn21pUsAAAAJ&hl=zh-CN&oi=ao">
							Xiangxiang Chu</a>, <a href="https://scholar.google.com/citations?user=qq6hueYAAAAJ&hl=zh-CN&oi=ao">
							Haibing Ren</a>, <a href="https://scholar.google.com/citations?user=s5b7lU4AAAAJ&hl=zh-CN&oi=ao">
							Xiaolin Wei</a>, <a href="https://weidixie.github.io/">
							Weidi Xie</a>, <a href="http://forestlinma.com/">
							Lin Ma</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="promptdet/index.html">project page</a> /
              <a href="https://arxiv.org/abs/2203.16513">arXiv</a>
              <p></p>
              <p>We propose an open-vocabulary object detector PromptDet, which is able to detect novel categories without any manual annotations.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <img src="resources/aedet.png" alt="clean-usnob" width="180">
            </td>
            <td width="75%" valign="middle">
              <a href="aedet/index.html">
                <papertitle>AeDet: Azimuth-invariant Multi-view 3D Object Detection</papertitle>
              </a>
              <br>
                <strong>Chenjian Feng</strong>,
                <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl=zh-CN&oi=ao">Zequn Jie</a>,
                <a href="https://y-zhong.info/">Yujie Zhong</a>,
                <a href="https://scholar.google.com/citations?user=jn21pUsAAAAJ&hl=zh-CN&oi=ao">Xiangxiang Chu</a>,
                <a href="http://forestlinma.com/">Lin Ma</a>
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="aedet/index.html">project page</a> /
              <a href="https://arxiv.org/abs/2211.12501">arXiv</a>
              <p></p>
              <p>We propose an Azimuth-equivariant Detector (AeDet) that is able to perform azimuth-invariant multi-view 3D object detection.</p>
              <br>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="resources/instagen.png" alt="clean-usnob" width="180" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="InstaGen/index.html">
                <papertitle>InstaGen: Enhancing Object Detection by Training on Synthetic Dataset</papertitle>
              </a>
              <br>
              <strong>Chenjian Feng</strong>, <a href="https://y-zhong.info/">Yujie Zhong</a>, <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl=zh-CN&oi=ao">
							Zequn Jie</a>, <a href="https://weidixie.github.io/">
							Weidi Xie</a>, <a href="http://forestlinma.com/">
							Lin Ma</a>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="InstaGen/index.html">project page</a> /
              <a href="https://arxiv.org/abs/2402.05937">arXiv</a>
              <p></p>
              <p>We introduce a novel paradigm to enhance the ability of object detector by training on synthetic dataset generated from diffusion models.</p>
            </td>
          </tr>

        </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>All Research</heading>
            <!-- <p>
              
            </p> -->
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="resources/InstructVEDit.png" alt="clean-usnob" width="180" height="100">
          </td>
          <td width="75%" valign="middle">
            <a href="https://o937-blip.github.io/InstructVEdit">
              <papertitle>InstructVEdit: A Holistic Approach for Instructional Video Editing</papertitle>
            </a>
            <br>
            <a href="https://scholar.google.com/citations?user=I0aKo_4AAAAJ&hl=zh-CN&oi=sra">Chi Zhang</a>*, 
            <strong>Chengjian Feng</strong>*, 
            <a href="https://scholar.google.com/citations?user=gO4divAAAAAJ&hl=zh-CN&oi=sra">Feng Yan</a>, 
            <a href="https://scholar.google.com/citations?user=f8rAZ7MAAAAJ&hl=zh-CN&oi=sra">Qiming Zhang</a>, 
            <a href="https://scholar.google.com/citations?user=oYdxAkcAAAAJ&hl=zh-CN&oi=sra">Mingjin Zhang</a>, 
            <a href="https://y-zhong.info/">Yujie Zhong</a>,
            <a href="https://scholar.google.com/citations?user=9jH5v74AAAAJ&hl=zh-CN&oi=sra">Jing Zhang</a>, 
            <a href="http://forestlinma.com">Lin Ma</a>
            <br>
            (*Equal contribution)
            <br>
            <em>Preprint</em>, 2025
            <br>
            <a href="https://o937-blip.github.io/InstructVEdit">project page</a> /
            <a href="https://arxiv.org/abs/2503.17641">arXiv</a>
            <p></p>We present InstructVEdit, a holistic approach for instructional video editing that includes a robust dataset curation workflow, two architectural model improvements, and an iterative refinement strategy.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="resources/P3Nav.png" alt="clean-usnob" width="180" height="100">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2503.18525">
              <papertitle>P3Nav: A Unified Framework for Embodied Navigation Integrating Perception, Planning, and Prediction</papertitle>
            </a>
            <br>
            <a>Yufeng Zhong</a>*, 
            <strong>Chengjian Feng</strong>*, 
            <a href="https://scholar.google.com/citations?user=gO4divAAAAAJ&hl=zh-CN&oi=sra">Feng Yan</a>, 
            <a href="https://scholar.google.com/citations?user=LPaXZEUAAAAJ&hl=en">Fanfan Liu</a>,
            <a>Liming Zheng</a>, 
            <a href="http://forestlinma.com">Lin Ma</a>
            <br>
            (*Equal contribution)
            <br>
            <em>Preprint</em>, 2025
            <br>
            <a href="https://arxiv.org/abs/2503.18525">project page</a> /
            <a href="https://arxiv.org/abs/2503.18525">arXiv</a>
            <p></p>we introduce P3Nav, a unified framework that integrates Perception, Planning, and Prediction capabilities through Multitask Collaboration on navigation and embodied question answering tasks,</p>
          </td>
        </tr>
      
        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="resources/DriveMM.png" alt="clean-usnob" width="180" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://zhijian11.github.io/DriveMM">
            <papertitle>DriveMM: All-in-One Large Multimodal Model for Autonomous Driving</papertitle>
          </a>
          <br>
          <a href="https://zhijian11.github.io/">Zhijian Huang</a>*, 
          <strong>Chengjian Feng</strong>*, 
          <a href="https://scholar.google.com/citations?user=gO4divAAAAAJ&hl=zh-CN&oi=sra">Feng Yan</a>, 
          <a>Baihui Xiao</a>, 
          <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl=zh-CN&oi=ao">Zequn Jie</a>, 
          <a href="https://y-zhong.info/">Yujie Zhong</a>,
          <a href="https://lemondan.github.io">Xiaodan Liang</a>, 
          <a href="http://forestlinma.com">Lin Ma</a>
          <br>
          (*Equal contribution)
          <br>
          <em>Preprint</em>, 2024
          <br>
          <a href="https://zhijian11.github.io/DriveMM">project page</a> /
          <a href="https://arxiv.org/abs/2412.07689">arXiv</a>
          <p></p>We propose a novel all-in-one large multimodal model, DriveMM, robustly equipped with the general capabilities and the generalization ability.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="resources/RoboMM.png" alt="clean-usnob" width="180" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://robouniview.github.io/RoboMM.github.io">
            <papertitle>RoboMM: All-in-One Multimodal Large Model for Robotic Manipulation</papertitle>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=gO4divAAAAAJ&hl=zh-CN&oi=sra">Feng Yan</a>, 
          <a href="https://scholar.google.com/citations?user=LPaXZEUAAAAJ&hl=en">Fanfan Liu</a>, 
          <a>Liming Zheng</a>, 
          <a>Yufeng Zhong</a>, 
          <a>Yiyang Huang</a>, 
          <a>Zechao Guan</a>, 
          <strong>Chengjian Feng</strong>, 
          <a href="http://forestlinma.com/">Lin Ma</a>
          <br>
          <em>Preprint</em>, 2024
          <br>
          <a href="https://robouniview.github.io/RoboMM.github.io">project page</a> /
          <a href="https://arxiv.org/abs/2412.07215">arXiv</a>
          <p></p>We propose a multimodal robotic manipulation model, RoboMM, along with a comprehensive dataset, RoboData.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="resources/instagen.png" alt="clean-usnob" width="180" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="InstaGen/index.html">
            <papertitle>InstaGen: Enhancing Object Detection by Training on Synthetic Dataset</papertitle>
          </a>
          <br>
          <strong>Chenjian Feng</strong>, <a href="https://y-zhong.info/">Yujie Zhong</a>, <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl=zh-CN&oi=ao">
          Zequn Jie</a>, <a href="https://weidixie.github.io/">
          Weidi Xie</a>, <a href="http://forestlinma.com/">
          Lin Ma</a>
          <br>
          <em>CVPR</em>, 2024
          <br>
          <a href="InstaGen/index.html">project page</a> /
          <a href="https://arxiv.org/abs/2402.05937">arXiv</a>
          <p></p>
          <p>We introduce a novel paradigm to enhance the ability of object detector by training on synthetic dataset generated from diffusion models.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="resources/UniMD.png" alt="clean-usnob" width="180" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://github.com/yingsen1/UniMD">
            <papertitle>UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection</papertitle>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=VKbjed8AAAAJ&hl=zh-CN&oi=sra">Yingsen Zeng</a>, 
          <a href="https://y-zhong.info/">Yujie Zhong</a>, 
          <strong>Chenjian Feng</strong>,
          <a href="http://forestlinma.com/">Lin Ma</a>
          <br>
          <em>ECCV</em>, 2024
          <br>
          <a href="https://github.com/yingsen1/UniMD">project page</a> /
          <a href="https://arxiv.org/abs/2404.04933">arXiv</a>
          <p></p>
          <p>We propose a unified architecture, UniMD, for both TAD and MR, and explore a task fusion learning scheme to enhance the mutual benefits between the two tasks.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="resources/AutoM3L.png" alt="clean-usnob" width="180" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://github.com/tim120526/AutoM3L">
            <papertitle>AutoM3L: An Automated Multimodal Machine Learning Framework with Large Language Models</papertitle>
          </a>
          <br>
          <a>Daqin Luo</a>, 
          <strong>Chenjian Feng</strong>,
          <a>Yuxuan Nong</a>, 
          <a href="https://scholar.google.com/citations?user=YDlF4lQAAAAJ&hl=zh-CN&oi=sra">Yiqing Shen</a>
          <br>
          <em>ACMMM</em>, 2024
          <br>
          <a href="https://github.com/tim120526/AutoM3L">project page</a> /
          <a href="https://arxiv.org/pdf/2408.00665">arXiv</a>
          <p></p>
          <p>We introduce AutoM3L, an innovative Automated Multimodal Machine Learning framework that leverages LLMs as controllers to automatically construct multimodal training pipelines.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="resources/RFSR.png" alt="clean-usnob" width="180" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://github.com/sxpro/RFSR">
            <papertitle>RFSR: Improving ISR Diffusion Models via Reward Feedback Learning</papertitle>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=MumqpkkAAAAJ&hl=zh-CN&oi=sra">Xiaopeng Sun</a>, 
          <a href="https://scholar.google.com/citations?user=3UefEmMAAAAJ&hl=zh-CN&oi=sra">Qinwei Lin</a>, 
          <a>Yu Gao</a>, 
          <a href="https://y-zhong.info/">Yujie Zhong</a>, 
          <strong>Chengjian Feng</strong>, 
          <a>Dengjie Li</a>, 
          <a>Zheng Zhao</a>, 
          <a>Jie Hu</a>, 
          <a href="http://forestlinma.com/">Lin Ma</a>
          <br>
          <em>Preprint</em>, 2024
          <br>
          <a href="https://github.com/sxpro/RFSR">project page</a> /
          <a href="https://arxiv.org/abs/2412.03268">arXiv</a>
          <p></p>We propose a multimodal robotic manipulation model, RoboMM, along with a comprehensive dataset, RoboData.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="resources/OV-DINO.png" alt="clean-usnob" width="180" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://github.com/wanghao9610/OV-DINO">
            <papertitle>OV-DINO: Unified Open-Vocabulary Detection with Language-Aware Selective Fusion</papertitle>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=PZajgHYAAAAJ&hl=zh-CN&oi=sra">Hao Wang</a>, 
          <a href="https://scholar.google.com/citations?user=yVxSn70AAAAJ&hl=zh-CN&oi=sra">Pengzhen Ren</a>, 
          <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl=zh-CN&oi=sra">Zequn Jie</a>, 
          <a href="https://scholar.google.com/citations?user=K7hCVDoAAAAJ&hl=zh-CN&oi=sra">Xiao Dong</a>, 
          <strong>Chenjian Feng</strong>, 
          <a>Yinlong Qian</a>, 
          <a href="http://forestlinma.com/">Lin Ma</a>, 
          <a>Dongmei Jiang,</a>, 
          <a href="https://scholar.google.com/citations?user=o_DllmIAAAAJ&hl=zh-CN&oi=sra">Yaowei Wang</a>, 
          <a href="https://scholar.google.com/citations?user=c3iwWRcAAAAJ&hl=zh-CN&oi=sra">Xiangyuan Lan</a>, 
          <a href="https://lemondan.github.io/">Xiaodan Liang</a>
          <br>
          <em>Preprint</em>, 2024
          <br>
          <a href="https://github.com/wanghao9610/OV-DINO">project page</a> /
          <a href="https://arxiv.org/abs/2407.07844">arXiv</a>
          <p></p>
          <p>We propose a novel unified open-vocabulary detection method called OV-DINO, which is pre-trained on diverse large-scale datasets with language-aware selective fusion in a unified framework.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="resources/RoboCAS.png" alt="clean-usnob" width="180" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://github.com/notFoundThisPerson/RoboCAS-v0">
            <papertitle>RoboCAS: A Benchmark for Robotic Manipulation in Complex Object Arrangement Scenarios</papertitle>
          </a>
          <br>
          <a>Liming Zheng</a>, 
          <a href="https://scholar.google.com/citations?user=gO4divAAAAAJ&hl=zh-CN&oi=sra">Feng Yan</a>, 
          <a href="https://scholar.google.com/citations?user=LPaXZEUAAAAJ&hl=en">Fanfan Liu</a>, 
          <strong>Chenjian Feng</strong>, 
          <a>Zhuoliang Kang</a>, 
          <a href="http://forestlinma.com/">Lin Ma</a>, 
          <br>
          <em>Preprint</em>, 2024
          <br>
          <a href="https://github.com/notFoundThisPerson/RoboCAS-v0">project page</a> /
          <a href="https://arxiv.org/abs/2407.06951">arXiv</a>
          <p></p>
          <p>We introduces RoboCAS, the first benchmark specifically designed for complex object arrangement scenarios in robotic manipulation.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="resources/RoboUniView.png" alt="clean-usnob" width="180" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://github.com/liufanfanlff/RoboUniview">
            <papertitle>RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulation</papertitle>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=LPaXZEUAAAAJ&hl=en">Fanfan Liu</a>, 
          <a href="https://scholar.google.com/citations?user=gO4divAAAAAJ&hl=zh-CN&oi=sra">Feng Yan</a>, 
          <a>Liming Zheng</a>, 
          <strong>Chenjian Feng</strong>, 
          <a>Yiyang Huang</a>, 
          <a href="http://forestlinma.com/">Lin Ma</a>, 
          <br>
          <em>Preprint</em>, 2024
          <br>
          <a href="https://github.com/liufanfanlff/RoboUniview">project page</a> /
          <a href="https://arxiv.org/abs/2406.18977">arXiv</a>
          <p></p>
          <p>We propose RoboUniView, an innovative approach that decouples visual feature extraction from action learning by a unified view representation.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:top">
          <img src="resources/aedet.png" alt="clean-usnob" width="180">
        </td>
        <td width="75%" valign="middle">
          <a href="aedet/index.html">
            <papertitle>AeDet: Azimuth-invariant Multi-view 3D Object Detection</papertitle>
          </a>
          <br>
            <strong>Chenjian Feng</strong>,
            <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl=zh-CN&oi=ao">Zequn Jie</a>,
            <a href="https://y-zhong.info/">Yujie Zhong</a>,
            <a href="https://scholar.google.com/citations?user=jn21pUsAAAAJ&hl=zh-CN&oi=ao">Xiangxiang Chu</a>,
            <a href="http://forestlinma.com/">Lin Ma</a>
          <br>
          <em>CVPR</em>, 2023
          <br>
          <a href="aedet/index.html">project page</a> /
          <a href="https://arxiv.org/abs/2211.12501">arXiv</a>
          <p></p>
          <p>We propose an Azimuth-equivariant Detector (AeDet) that is able to perform azimuth-invariant multi-view 3D object detection.</p>
          <br>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="resources/FastPillars.png" alt="clean-usnob" width="180" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://github.com/StiphyJay/FastPillars">
            <papertitle>FastPillars: A Deployment-friendly Pillar-based 3D Detector</papertitle>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=kSdqoi0AAAAJ&hl=zh-CN&oi=sra">Sifan Zhou</a>, 
          <a href="https://scholar.google.com/citations?user=xSF3BBoAAAAJ&hl=zh-CN&oi=sra">Zhi Tian</a>, 
          <a href="https://scholar.google.com/citations?user=jn21pUsAAAAJ&hl=zh-CN&oi=ao">Xiangxiang Chu</a>, 
          <a href="https://scholar.google.com/citations?user=zGLVABAAAAAJ&hl=zh-CN&oi=sra">Xinyu Zhang</a>, 
          <a href="https://scholar.google.com/citations?user=uUNQnu0AAAAJ&hl=zh-CN&oi=sra">Bo Zhang</a>, 
          <a>Xiaobo Lu</a>, 
          <strong>Chenjian Feng</strong>, 
          <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl=zh-CN&oi=ao">Zequn Jie</a>, 
          <a href="https://scholar.google.com/citations?user=4nYbZ0YAAAAJ&hl=zh-CN&oi=sra">Patrick Yin Chiang</a>, 
          <a href="http://forestlinma.com/">Lin Ma</a>
          <br>
          <em>Preprint</em>, 2023
          <br>
          <a href="https://github.com/StiphyJay/FastPillars">project page</a> /
          <a href="https://arxiv.org/abs/2302.02367">arXiv</a>
          <p></p>
          <p>We devise a deployment-friendly pillar-based 3D detector, termed FastPillars, to tackle the challenge of efficient 3D object detection from an industry perspective.</p>
        </td>
      </tr>

      <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="resources/promptdet.png" alt="clean-usnob" width="180">
          </td>
          <td width="75%" valign="middle">
            <a href="promptdet/index.html">
              <papertitle>PromptDet: Towards Open-vocabulary Detection using Uncurated Images</papertitle>
            </a>
            <br>
            <strong>Chenjian Feng</strong>, <a href="https://y-zhong.info/">Yujie Zhong</a>, <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl=zh-CN&oi=ao">
            Zequn Jie</a>, <a href="https://scholar.google.com/citations?user=jn21pUsAAAAJ&hl=zh-CN&oi=ao">
            Xiangxiang Chu</a>, <a href="https://scholar.google.com/citations?user=qq6hueYAAAAJ&hl=zh-CN&oi=ao">
            Haibing Ren</a>, <a href="https://scholar.google.com/citations?user=s5b7lU4AAAAJ&hl=zh-CN&oi=ao">
            Xiaolin Wei</a>, <a href="https://weidixie.github.io/">
            Weidi Xie</a>, <a href="http://forestlinma.com/">
            Lin Ma</a>
            <br>
            <em>ECCV</em>, 2022
            <br>
            <a href="promptdet/index.html">project page</a> /
            <a href="https://arxiv.org/abs/2203.16513">arXiv</a>
            <p></p>
            <p>We propose an open-vocabulary object detector PromptDet, which is able to detect novel categories without any manual annotations.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="resources/tood.png" alt="clean-usnob" width="180">
          </td>
          <td width="75%" valign="middle">
            <a href="tood/index.html">
              <papertitle>TOOD: Task-aligned One-stage Object Detection</papertitle>
            </a>
            <br>
            <strong>Chenjian Feng</strong>, <a href="https://y-zhong.info/">Yujie Zhong</a>, <a>Yu Gao</a>, <a href="https://scholar.google.com/citations?user=0isiW-4AAAAJ&hl=zh-CN&oi=ao">Matthew R. Scott</a>, <a href="http://whuang.org/">Weilin Huang</a>
            <br>
            <em>ICCV</em>, 2021
            <font color="red"><strong>(Oral)</strong></font>
            <br>
            <a href="tood/index.html">project page</a> /
            <a href="https://arxiv.org/abs/2108.07755">arXiv</a>
            <p></p>
            <p>We propose a Task-aligned One-stage Object Detection (TOOD) that explicitly aligns the classification and localization tasks in a learning-based manner.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="resources/loce.png" alt="clean-usnob" width="180">
          </td>
          <td width="75%" valign="middle">
            <a href="loce/index.html">
              <papertitle>Exploring Classification Equilibrium in Long-Tailed Object Detection</papertitle>
            </a>
            <br>
            <strong>Chenjian Feng</strong>, <a href="https://y-zhong.info/">Yujie Zhong</a>, <a href="http://whuang.org/">Weilin Huang</a>
            <br>
            <em>ICCV</em>, 2021
            <br>
            <a href="loce/index.html">project page</a> /
            <a href="https://arxiv.org/abs/2108.07507">arXiv</a>
            <p></p>
            <p>We balance the classification of the long-tailed detector via an Equilibrium Loss (EBL) and a Memory-augmented Feature Sampling (MFS) method.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="resources/sbada_gan.png" alt="clean-usnob" width="180" height="100">
          </td>
          <td width="75%" valign="middle">
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231219304941">
              <papertitle>Domain adaptation with SBADA-GAN and Mean Teacher</papertitle>
            </a>
            <br>
            <strong>Chenjian Feng</strong>, <a>Zhaoshui He</a>, <a href="https://scholar.google.com/citations?user=ijtp4KIAAAAJ&hl=zh-CN&oi=sra">Jiawei Wang</a>, <a>Qinzhuang Lin</a>, <a>Zhouping Zhu</a>, <a>Jun Lv</a>, <a>Shengli Xie</a>
            <br>
            <em>Neurocomputing</em>, 2020
            <br>
            <!--a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231219304941">paper</a-->
            <p></p>
            <p>We propose a powerful model for unsupervised domain adaptation by introducing  Mean Teacher as a target classifier of SBADA-GAN.</p>
          </td>
        </tr>

      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template gratefully stolen from <a href="https://github.com/jonbarron/website/">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
